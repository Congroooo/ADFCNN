#è®­ç»ƒå‘½ä»¤
## è®­ç»ƒå—è¯•è€… S02ï¼Œä½¿ç”¨GPU 1ï¼Œåªè·‘fold 1
# python new_training.py --subject_num 0 --gpu_num 0 --fold_num 1

'''Import libraires'''
import os, yaml
from datetime import datetime

import pandas as pd
from easydict import EasyDict
import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader
import braindecode.preprocessing.preprocess

from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import TQDMProgressBar
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold
from dataloader.bci_compet import get_dataset
from model.litmodel import get_litmodel
from utils.setup_utils import (
    get_device,
    get_log_name,
)
from utils.training_utils import get_callbacks

'''Argparse'''
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--subject_num', type=int, default=17)
parser.add_argument('--fold_num', type=int, default=0)
parser.add_argument('--gpu_num', type=str, default='0')
parser.add_argument('--config_name', type=str, default='bcicompet2a_config')
aargs = parser.parse_args()

# GPU_check - åªä¿ç•™åŸºæœ¬ä¿¡æ¯
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
print("=" * 50)

# Config setting
with open('B:/AI/python_project/ADFCNN-MI/ADFCNN/configs/bcicompet2a_config.yaml') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
    args = EasyDict(config)

#### Set SEED ####
seed_everything(args.SEED)

#### Set Device ####
if torch.cuda.is_available():
    os.environ['CUDA_VISIBLE_DEVICES'] = aargs.gpu_num
args['device'] = get_device(aargs.gpu_num)
cudnn.benchmark = True
cudnn.fastest = True
cudnn.deterministic = True

#### Set Log ####
args['current_time'] = datetime.now().strftime('%Y%m%d')
args['LOG_NAME'] = get_log_name(args)

#### Update configs ####
args.lr = float(args.lr)
if args.downsampling != 0: args['sampling_rate'] = args.downsampling

# åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
plot_dir = os.path.join(args.LOG_PATH, 'training_plots')
os.makedirs(plot_dir, exist_ok=True)

# å­˜å‚¨æ‰€æœ‰ç»“æœ
all_results = []


def generate_plot_from_model_history(model, subject_num, fold_num, plot_dir):
    """ç›´æ¥ä»æ¨¡å‹è®­ç»ƒå†å²ç”Ÿæˆå›¾è¡¨"""
    try:
        if not hasattr(model, 'training_history'):
            print("âŒ æ¨¡å‹æ²¡æœ‰training_historyå±æ€§")
            return None

        train_loss = model.training_history['train_loss']
        train_acc = model.training_history['train_acc']
        val_loss = model.training_history['val_loss']
        val_acc = model.training_history['val_acc']

        print(
            f"ğŸ“Š æ•°æ®é•¿åº¦ - train_loss: {len(train_loss)}, val_loss: {len(val_loss)}, train_acc: {len(train_acc)}, val_acc: {len(val_acc)}")

        if not train_loss:
            print("âš ï¸ è®­ç»ƒå†å²ä¸ºç©º")
            return None

        # åˆ›å»ºä¸¤ä¸ªå•ç‹¬çš„å›¾è¡¨
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        fig2, ax2 = plt.subplots(figsize=(10, 6))

        # ç¡®å®šå…±åŒçš„æ•°æ®é•¿åº¦ï¼ˆå–æœ€å°å€¼ï¼‰
        min_length = min(len(train_loss), len(val_loss) if val_loss else len(train_loss))

        # å¦‚æœéªŒè¯æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®é•¿åº¦
        if not val_loss:
            min_length = len(train_loss)

        epochs = range(1, min_length + 1)

        # å›¾è¡¨1: å‡†ç¡®ç‡æ›²çº¿
        if len(train_acc) >= min_length:
            ax1.plot(epochs, train_acc[:min_length], 'b-', label='Train Accuracy', linewidth=2, marker='o',
                     markersize=3)

        if val_acc and len(val_acc) >= min_length:
            ax1.plot(epochs, val_acc[:min_length], 'r-', label='Validation Accuracy', linewidth=2, marker='s',
                     markersize=3)
            print(f"âœ… ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡æ›²çº¿ï¼Œæ•°æ®ç‚¹: {min_length}")
        elif val_acc:
            print(f"âš ï¸ éªŒè¯å‡†ç¡®ç‡æ•°æ®ä¸è¶³: {len(val_acc)} < {min_length}")

        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Accuracy')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)  # å‡†ç¡®ç‡èŒƒå›´0-1

        # å›¾è¡¨2: æŸå¤±æ›²çº¿
        if len(train_loss) >= min_length:
            ax2.plot(epochs, train_loss[:min_length], 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)

        if val_loss and len(val_loss) >= min_length:
            ax2.plot(epochs, val_loss[:min_length], 'r-', label='Validation Loss', linewidth=2, marker='s',
                     markersize=3)
            print(f"âœ… ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿ï¼Œæ•°æ®ç‚¹: {min_length}")
        elif val_loss:
            print(f"âš ï¸ éªŒè¯æŸå¤±æ•°æ®ä¸è¶³: {len(val_loss)} < {min_length}")

        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # ä¿å­˜ä¸¤ä¸ªå›¾è¡¨
        acc_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_accuracy.png')
        loss_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_loss.png')

        fig1.tight_layout()
        fig1.savefig(acc_plot_path, dpi=300, bbox_inches='tight')
        plt.close(fig1)

        fig2.tight_layout()
        fig2.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close(fig2)

        print(f"ğŸ“Š ä»æ¨¡å‹å†å²ç”Ÿæˆè®­ç»ƒæ›²çº¿:")
        print(f"   - å‡†ç¡®ç‡å›¾è¡¨: {acc_plot_path}")
        print(f"   - æŸå¤±å›¾è¡¨: {loss_plot_path}")

        return [acc_plot_path, loss_plot_path]

    except Exception as e:
        print(f"âŒ ä»æ¨¡å‹å†å²ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

def plot_training_curves_from_csv(csv_path, subject_num, fold_num, plot_dir):
    """ä»TensorBoard CSVæ—¥å¿—ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        if not os.path.exists(csv_path):
            print(f"âš ï¸ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return None

        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path)

        # åˆ›å»ºä¸¤ä¸ªå›¾è¡¨ï¼šä¸€ä¸ªç”¨äºå‡†ç¡®ç‡ï¼Œä¸€ä¸ªç”¨äºæŸå¤±
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        fig2, ax2 = plt.subplots(figsize=(10, 6))

        # å›¾è¡¨1: å‡†ç¡®ç‡æ›²çº¿ (train_acc å’Œ val_acc åœ¨ä¸€èµ·)
        if 'train_acc_epoch' in df.columns and 'val_acc_epoch' in df.columns:
            train_acc = df['train_acc_epoch'].dropna().values
            val_acc = df['val_acc_epoch'].dropna().values
            epochs = range(1, len(train_acc) + 1)

            ax1.plot(epochs, train_acc, 'b-', label='Train Accuracy', linewidth=2, marker='o', markersize=3)
            ax1.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=3)
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Accuracy')
            ax1.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Accuracy')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            ax1.set_ylim(0, 1)  # å‡†ç¡®ç‡èŒƒå›´0-1

            # ä¿å­˜å‡†ç¡®ç‡å›¾è¡¨
            acc_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_accuracy.png')
            fig1.tight_layout()
            fig1.savefig(acc_plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig1)
            print(f"ğŸ“Š å‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜: {acc_plot_path}")

        # å›¾è¡¨2: æŸå¤±æ›²çº¿ (train_loss å’Œ val_loss åœ¨ä¸€èµ·)
        if 'train_loss_epoch' in df.columns and 'val_loss_epoch' in df.columns:
            train_loss = df['train_loss_epoch'].dropna().values
            val_loss = df['val_loss_epoch'].dropna().values
            epochs = range(1, len(train_loss) + 1)

            ax2.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)
            ax2.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=3)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Loss')
            ax2.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Loss')
            ax2.legend()
            ax2.grid(True, alpha=0.3)

            # ä¿å­˜æŸå¤±å›¾è¡¨
            loss_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_loss.png')
            fig2.tight_layout()
            fig2.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig2)
            print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_plot_path}")

        return [acc_plot_path, loss_plot_path]

    except Exception as e:
        print(f"âŒ ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
        return None


'''Training'''
print(f"ğŸ“‹ æ€»å—è¯•è€…æ•°: {args.num_subjects}, ç›®æ ‡å—è¯•è€…: {aargs.subject_num}")
print(f"ğŸ“‹ æ€»Foldæ•°: {args.k_folds}, ç›®æ ‡Fold: {aargs.fold_num}")

for num_subject in range(args.num_subjects):
    # ä¿®æ”¹é€‰æ‹©é€»è¾‘ï¼šå¦‚æœsubject_numæ˜¯é»˜è®¤å€¼17ï¼Œå°±è®­ç»ƒæ‰€æœ‰å—è¯•è€…
    if aargs.subject_num != 17 and num_subject != aargs.subject_num:
        print(f"â­ï¸  è·³è¿‡å—è¯•è€… {num_subject}")
        continue

    args['target_subject'] = num_subject
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå—è¯•è€… S{num_subject:02d}")

    dataset = get_dataset(aargs.config_name, args)

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(dataset) == 0:
        print(f"âŒ å—è¯•è€… S{num_subject:02d} æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡")
        continue

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")

    kfold = KFold(n_splits=args.k_folds, shuffle=True, random_state=args.SEED)

    for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):
        # ä¿®æ”¹é€‰æ‹©é€»è¾‘ï¼šå¦‚æœfold_numæ˜¯é»˜è®¤å€¼0ï¼Œå°±è®­ç»ƒæ‰€æœ‰fold
        if aargs.fold_num != 0 and fold != aargs.fold_num:
            print(f"â­ï¸  è·³è¿‡Fold {fold}")
            continue

        print(f"ğŸ”„ Fold {fold + 1}/{args.k_folds}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_idx)}, éªŒè¯æ ·æœ¬: {len(val_idx)}")

        ### Set dataloader ###
        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)
        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)

        train_dataloader = DataLoader(dataset,
                                      batch_size=args.batch_size,
                                      pin_memory=False,
                                      num_workers=args.num_workers,
                                      sampler=train_subsampler)
        val_dataloader = DataLoader(dataset,
                                    batch_size=args.batch_size,
                                    pin_memory=False,
                                    num_workers=args.num_workers,
                                    sampler=val_subsampler)

        model = get_litmodel(args)
        logger = TensorBoardLogger(args.LOG_PATH,
                                   name=f'{args.LOG_NAME}/S{args.target_subject:02d}_fold{fold + 1}',version='')
        callbacks = get_callbacks(fold=fold, monitor='val_acc', args=args)

        # è‡ªå®šä¹‰è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºæ‰€æœ‰æŒ‡æ ‡
        # æ›¿æ¢åŸæ¥çš„ DetailedProgressBar ç±»
        class DetailedProgressBar(TQDMProgressBar):
            def init_validation_tqdm(self):
                bar = super().init_validation_tqdm()
                bar.disable = True  # ç¦ç”¨éªŒè¯è¿›åº¦æ¡
                return bar

            def get_metrics(self, trainer, model):
                items = super().get_metrics(trainer, model)
                # ç§»é™¤v_numï¼Œæ·»åŠ è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
                items.pop("v_num", None)

                # æ·»åŠ è®­ç»ƒæŒ‡æ ‡
                if 'train_loss' in trainer.callback_metrics:
                    items['train_loss'] = f"{trainer.callback_metrics['train_loss']:.4f}"
                if 'train_acc' in trainer.callback_metrics:
                    items['train_acc'] = f"{trainer.callback_metrics['train_acc']:.4f}"

                # æ·»åŠ éªŒè¯æŒ‡æ ‡
                if 'val_loss' in trainer.callback_metrics:
                    items['val_loss'] = f"{trainer.callback_metrics['val_loss']:.4f}"
                if 'val_acc' in trainer.callback_metrics:
                    items['val_acc'] = f"{trainer.callback_metrics['val_acc']:.4f}"

                return items
        callbacks.append(DetailedProgressBar())

        trainer = Trainer(
            enable_progress_bar=True,
            max_epochs=args.EPOCHS,
            accelerator="gpu" if aargs.gpu_num else "cpu",
            devices=[int(aargs.gpu_num)] if aargs.gpu_num else None,
            callbacks=callbacks,
            default_root_dir=args.CKPT_PATH,
            logger=logger,
            log_every_n_steps=20,  # å¢åŠ æ—¥å¿—é¢‘ç‡ä»¥è·å–æ›´å¤šè®­ç»ƒç‚¹
        )

        # è®­ç»ƒå¹¶è·å–æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        trainer.fit(model,
                    train_dataloaders=train_dataloader,
                    val_dataloaders=val_dataloader)

        # è·å–æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_val_acc = trainer.checkpoint_callback.best_model_score.item()

        # ç”Ÿæˆè®­ç»ƒæ›²çº¿
        try:
            # ä¿®æ­£TensorBoardæ—¥å¿—è·¯å¾„æŸ¥æ‰¾
            log_dir = logger.log_dir
            print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")

            # æŸ¥æ‰¾metrics.csvæ–‡ä»¶
            csv_path = None
            for root, dirs, files in os.walk(log_dir):
                if 'metrics.csv' in files:
                    csv_path = os.path.join(root, 'metrics.csv')
                    break

            plot_paths = []
            if csv_path and os.path.exists(csv_path):
                print(f"âœ… æ‰¾åˆ°CSVæ–‡ä»¶: {csv_path}")
                plot_paths = plot_training_curves_from_csv(csv_path, num_subject, fold + 1, plot_dir)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°CSVæ—¥å¿—æ–‡ä»¶ï¼Œæœç´¢ç›®å½•: {log_dir}")
                # å°è¯•ç›´æ¥ä»æ¨¡å‹å†å²ç”Ÿæˆå›¾è¡¨
                plot_paths = generate_plot_from_model_history(model, num_subject, fold + 1, plot_dir)

        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆè®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
            plot_paths = []

        all_results.append({
            'subject': num_subject,
            'fold': fold,
            'val_acc': best_val_acc,
            'plot_path': plot_paths
        })

        print(f"âœ… å—è¯•è€… S{num_subject:02d} Fold {fold + 1} å®Œæˆ")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

        torch.cuda.empty_cache()

# æ˜¾ç¤ºæ±‡æ€»ç»“æœ
print("\n" + "=" * 60)
print("ğŸ¯ è®­ç»ƒç»“æœæ±‡æ€»")
print("=" * 60)

if all_results:
    for result in all_results:
        print(f"å—è¯•è€… S{result['subject']:02d} Fold {result['fold'] + 1}: {result['val_acc']:.4f}")
        if result['plot_path']:
            print(f"   å›¾è¡¨è·¯å¾„: {result['plot_path']}")

    # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
    avg_acc = sum(r['val_acc'] for r in all_results) / len(all_results)
    print(f"\nğŸ“ˆ å¹³å‡éªŒè¯å‡†ç¡®ç‡: {avg_acc:.4f}")

    # ç”Ÿæˆæ±‡æ€»å›¾è¡¨
    print(f"\nğŸ“ æ‰€æœ‰è®­ç»ƒå›¾è¡¨ä¿å­˜åœ¨: {plot_dir}")
else:
    print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœ")

print("=" * 60)