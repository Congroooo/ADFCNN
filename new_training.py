# è®­ç»ƒå‘½ä»¤
## è®­ç»ƒå—è¯•è€… S02ï¼Œä½¿ç”¨GPU 1ï¼Œåªè·‘fold 1
# python new_training.py --subject_num 0 --gpu_num 0 --fold_num 1

'''Import libraires'''
import os, yaml
from datetime import datetime

import pandas as pd
from easydict import EasyDict
import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader

# æ·»åŠ NumPyå…¼å®¹æ€§ä¿®å¤
import numpy as np

try:
    # å°è¯•ä¿®å¤NumPyçš„clipé—®é¢˜
    original_clip = np.clip


    def fixed_clip(a, a_min, a_max, out=None, **kwargs):
        if out is not None and out.dtype != np.float64:
            out = None  # é¿å…dtypeä¸åŒ¹é…
        return original_clip(a, a_min, a_max, out=out, **kwargs)


    np.clip = fixed_clip
except Exception as e:
    print(f"NumPy clipä¿®å¤å¤±è´¥: {e}")

# ç°åœ¨å¯¼å…¥å…¶ä»–åº“
import braindecode.preprocessing.preprocess

from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import TQDMProgressBar
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold
from sklearn.metrics import cohen_kappa_score, confusion_matrix
from dataloader.bci_compet import get_dataset
from model.litmodel import get_litmodel
from utils.setup_utils import (
    get_device,
    get_log_name,
)
from utils.training_utils import get_callbacks

'''Argparse'''
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--subject_num', type=int, default=17)
parser.add_argument('--fold_num', type=int, default=0)
parser.add_argument('--gpu_num', type=str, default='0')
parser.add_argument('--config_name', type=str, default='bcicompet2a_config')
aargs = parser.parse_args()

# GPU_check - åªä¿ç•™åŸºæœ¬ä¿¡æ¯
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
else:
    print("â„¹ï¸  ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
print("=" * 50)

# Config setting
with open('D:/python_project/ADFCNN-main/configs/bcicompet2a_config.yaml') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
    args = EasyDict(config)

#### Set SEED ####
seed_everything(args.SEED)

#### Set Device ####
# æ ¹æ®CUDAå¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
use_gpu = torch.cuda.is_available() and aargs.gpu_num
if use_gpu:
    os.environ['CUDA_VISIBLE_DEVICES'] = aargs.gpu_num
    args['device'] = get_device(aargs.gpu_num)
    cudnn.benchmark = True
    cudnn.fastest = True
    cudnn.deterministic = True
    print(f"âœ… ä½¿ç”¨GPU {aargs.gpu_num} è¿›è¡Œè®­ç»ƒ")
else:
    args['device'] = torch.device('cpu')
    print("â„¹ï¸  ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")

#### Set Log ####
args['current_time'] = datetime.now().strftime('%Y%m%d')
args['LOG_NAME'] = get_log_name(args)

#### Update configs ####
args.lr = float(args.lr)
if args.downsampling != 0: args['sampling_rate'] = args.downsampling

# åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
plot_dir = os.path.join(args.LOG_PATH, 'training_plots')
os.makedirs(plot_dir, exist_ok=True)

# å­˜å‚¨æ‰€æœ‰ç»“æœ
all_results = []


def generate_plot_from_model_history(model, subject_num, fold_num, plot_dir):
    """ç›´æ¥ä»æ¨¡å‹è®­ç»ƒå†å²ç”Ÿæˆå›¾è¡¨ï¼ˆåŒ…å«Kappaï¼‰"""
    try:
        if not hasattr(model, 'training_history'):
            print("âŒ æ¨¡å‹æ²¡æœ‰training_historyå±æ€§")
            return None

        train_loss = model.training_history['train_loss']
        train_acc = model.training_history['train_acc']
        val_loss = model.training_history['val_loss']
        val_acc = model.training_history['val_acc']

        # æ–°å¢Kappaæ•°æ®
        train_kappa = model.training_history.get('train_kappa', [])
        val_kappa = model.training_history.get('val_kappa', [])

        print(
            f"ğŸ“Š æ•°æ®é•¿åº¦ - train_loss: {len(train_loss)}, val_loss: {len(val_loss)}, train_acc: {len(train_acc)}, val_acc: {len(val_acc)}")
        if train_kappa:
            print(f"ğŸ“Š Kappaé•¿åº¦ - train_kappa: {len(train_kappa)}, val_kappa: {len(val_kappa)}")

        if not train_loss:
            print("âš ï¸ è®­ç»ƒå†å²ä¸ºç©º")
            return None

        # åˆ›å»ºä¸‰ä¸ªå•ç‹¬çš„å›¾è¡¨ï¼šå‡†ç¡®ç‡ã€æŸå¤±ã€Kappa
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        fig3, ax3 = plt.subplots(figsize=(10, 6))

        # ç¡®å®šå…±åŒçš„æ•°æ®é•¿åº¦ï¼ˆå–æœ€å°å€¼ï¼‰
        min_length = min(len(train_loss), len(val_loss) if val_loss else len(train_loss))

        # å¦‚æœéªŒè¯æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®é•¿åº¦
        if not val_loss:
            min_length = len(train_loss)

        epochs = range(1, min_length + 1)

        # å›¾è¡¨1: å‡†ç¡®ç‡æ›²çº¿
        if len(train_acc) >= min_length:
            ax1.plot(epochs, train_acc[:min_length], 'b-', label='Train Accuracy', linewidth=2, marker='o',
                     markersize=3)

        if val_acc and len(val_acc) >= min_length:
            ax1.plot(epochs, val_acc[:min_length], 'r-', label='Validation Accuracy', linewidth=2, marker='s',
                     markersize=3)
            print(f"âœ… ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡æ›²çº¿ï¼Œæ•°æ®ç‚¹: {min_length}")
        elif val_acc:
            print(f"âš ï¸ éªŒè¯å‡†ç¡®ç‡æ•°æ®ä¸è¶³: {len(val_acc)} < {min_length}")

        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Accuracy')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)  # å‡†ç¡®ç‡èŒƒå›´0-1

        # å›¾è¡¨2: æŸå¤±æ›²çº¿
        if len(train_loss) >= min_length:
            ax2.plot(epochs, train_loss[:min_length], 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)

        if val_loss and len(val_loss) >= min_length:
            ax2.plot(epochs, val_loss[:min_length], 'r-', label='Validation Loss', linewidth=2, marker='s',
                     markersize=3)
            print(f"âœ… ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿ï¼Œæ•°æ®ç‚¹: {min_length}")
        elif val_loss:
            print(f"âš ï¸ éªŒè¯æŸå¤±æ•°æ®ä¸è¶³: {len(val_loss)} < {min_length}")

        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # å›¾è¡¨3: Kappaæ›²çº¿
        if train_kappa and len(train_kappa) >= min_length:
            ax3.plot(epochs, train_kappa[:min_length], 'b-', label='Train Kappa', linewidth=2, marker='o', markersize=3)

        if val_kappa and len(val_kappa) >= min_length:
            ax3.plot(epochs, val_kappa[:min_length], 'r-', label='Validation Kappa', linewidth=2, marker='s',
                     markersize=3)
            print(f"âœ… ç»˜åˆ¶éªŒè¯Kappaæ›²çº¿ï¼Œæ•°æ®ç‚¹: {min_length}")
        elif val_kappa:
            print(f"âš ï¸ éªŒè¯Kappaæ•°æ®ä¸è¶³: {len(val_kappa)} < {min_length}")

        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Kappa')
        ax3.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Kappa')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(-0.1, 1.0)  # KappaèŒƒå›´-1åˆ°1ï¼Œè¿™é‡Œè®¾ç½®-0.1åˆ°1.0

        # ä¿å­˜ä¸‰ä¸ªå›¾è¡¨
        acc_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_accuracy.png')
        loss_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_loss.png')
        kappa_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_kappa.png')

        fig1.tight_layout()
        fig1.savefig(acc_plot_path, dpi=300, bbox_inches='tight')
        plt.close(fig1)

        fig2.tight_layout()
        fig2.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close(fig2)

        fig3.tight_layout()
        fig3.savefig(kappa_plot_path, dpi=300, bbox_inches='tight')
        plt.close(fig3)

        print(f"ğŸ“Š ä»æ¨¡å‹å†å²ç”Ÿæˆè®­ç»ƒæ›²çº¿:")
        print(f"   - å‡†ç¡®ç‡å›¾è¡¨: {acc_plot_path}")
        print(f"   - æŸå¤±å›¾è¡¨: {loss_plot_path}")
        print(f"   - Kappaå›¾è¡¨: {kappa_plot_path}")

        return [acc_plot_path, loss_plot_path, kappa_plot_path]

    except Exception as e:
        print(f"âŒ ä»æ¨¡å‹å†å²ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def plot_training_curves_from_csv(csv_path, subject_num, fold_num, plot_dir):
    """ä»TensorBoard CSVæ—¥å¿—ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆåŒ…å«Kappaï¼‰"""
    try:
        if not os.path.exists(csv_path):
            print(f"âš ï¸ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return None

        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path)

        # åˆ›å»ºä¸‰ä¸ªå›¾è¡¨ï¼šå‡†ç¡®ç‡ã€æŸå¤±ã€Kappa
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        fig3, ax3 = plt.subplots(figsize=(10, 6))

        plot_paths = []

        # å›¾è¡¨1: å‡†ç¡®ç‡æ›²çº¿
        if 'train_acc_epoch' in df.columns and 'val_acc_epoch' in df.columns:
            train_acc = df['train_acc_epoch'].dropna().values
            val_acc = df['val_acc_epoch'].dropna().values
            epochs = range(1, len(train_acc) + 1)

            ax1.plot(epochs, train_acc, 'b-', label='Train Accuracy', linewidth=2, marker='o', markersize=3)
            ax1.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=3)
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Accuracy')
            ax1.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Accuracy')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            ax1.set_ylim(0, 1)

            # ä¿å­˜å‡†ç¡®ç‡å›¾è¡¨
            acc_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_accuracy.png')
            fig1.tight_layout()
            fig1.savefig(acc_plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig1)
            plot_paths.append(acc_plot_path)
            print(f"ğŸ“Š å‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜: {acc_plot_path}")

        # å›¾è¡¨2: æŸå¤±æ›²çº¿
        if 'train_loss_epoch' in df.columns and 'val_loss_epoch' in df.columns:
            train_loss = df['train_loss_epoch'].dropna().values
            val_loss = df['val_loss_epoch'].dropna().values
            epochs = range(1, len(train_loss) + 1)

            ax2.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)
            ax2.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=3)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Loss')
            ax2.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Loss')
            ax2.legend()
            ax2.grid(True, alpha=0.3)

            # ä¿å­˜æŸå¤±å›¾è¡¨
            loss_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_loss.png')
            fig2.tight_layout()
            fig2.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig2)
            plot_paths.append(loss_plot_path)
            print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_plot_path}")

        # å›¾è¡¨3: Kappaæ›²çº¿
        if 'train_kappa_epoch' in df.columns and 'val_kappa_epoch' in df.columns:
            train_kappa = df['train_kappa_epoch'].dropna().values
            val_kappa = df['val_kappa_epoch'].dropna().values
            epochs = range(1, len(train_kappa) + 1)

            ax3.plot(epochs, train_kappa, 'b-', label='Train Kappa', linewidth=2, marker='o', markersize=3)
            ax3.plot(epochs, val_kappa, 'r-', label='Validation Kappa', linewidth=2, marker='s', markersize=3)
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Kappa')
            ax3.set_title(f'Subject {subject_num:02d} Fold {fold_num} - Training vs Validation Kappa')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            ax3.set_ylim(-0.1, 1.0)

            # ä¿å­˜Kappaå›¾è¡¨
            kappa_plot_path = os.path.join(plot_dir, f'S{subject_num:02d}_fold{fold_num}_kappa.png')
            fig3.tight_layout()
            fig3.savefig(kappa_plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig3)
            plot_paths.append(kappa_plot_path)
            print(f"ğŸ“Š Kappaæ›²çº¿å·²ä¿å­˜: {kappa_plot_path}")
        else:
            print("â„¹ï¸  CSVä¸­æœªæ‰¾åˆ°Kappaæ•°æ®")

        return plot_paths

    except Exception as e:
        print(f"âŒ ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
        return None


def calculate_detailed_metrics(model, dataloader, device):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€Kappaã€æ··æ·†çŸ©é˜µï¼‰"""
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch in dataloader:
            x, y = batch['data'], batch['label']
            x = x.type(torch.float).to(device)
            y = y.type(torch.long) if y.dim() == 1 else y
            y = y.to(device)

            outputs = model(x)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            # å¤„ç†one-hotç¼–ç çš„æ ‡ç­¾
            if y.dim() > 1:
                y_labels = torch.argmax(y, dim=1)
            else:
                y_labels = y
            all_targets.extend(y_labels.cpu().numpy())

    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)

    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    accuracy = (all_preds == all_targets).mean()
    kappa = cohen_kappa_score(all_targets, all_preds)
    cm = confusion_matrix(all_targets, all_preds)

    return {
        'accuracy': accuracy,
        'kappa': kappa,
        'confusion_matrix': cm,
        'predictions': all_preds,
        'targets': all_targets
    }


def interpret_kappa(kappa_value):
    """è§£è¯»Kappaç³»æ•°"""
    if kappa_value < 0:
        return "ä¸€è‡´æ€§æ¯”éšæœºçŒœæµ‹è¿˜å·®"
    elif kappa_value <= 0.20:
        return "æä½çš„ä¸€è‡´æ€§"
    elif kappa_value <= 0.40:
        return "ä¸€èˆ¬çš„ä¸€è‡´æ€§"
    elif kappa_value <= 0.60:
        return "ä¸­ç­‰çš„ä¸€è‡´æ€§"
    elif kappa_value <= 0.80:
        return "é«˜åº¦çš„ä¸€è‡´æ€§"
    else:
        return "å‡ ä¹å®Œå…¨ä¸€è‡´"


'''Training'''
print(f"ğŸ“‹ æ€»å—è¯•è€…æ•°: {args.num_subjects}, ç›®æ ‡å—è¯•è€…: {aargs.subject_num}")
print(f"ğŸ“‹ æ€»Foldæ•°: {args.k_folds}, ç›®æ ‡Fold: {aargs.fold_num}")

for num_subject in range(args.num_subjects):
    # ä¿®æ”¹é€‰æ‹©é€»è¾‘ï¼šå¦‚æœsubject_numæ˜¯é»˜è®¤å€¼17ï¼Œå°±è®­ç»ƒæ‰€æœ‰å—è¯•è€…
    if aargs.subject_num != 17 and num_subject != aargs.subject_num:
        print(f"â­ï¸  è·³è¿‡å—è¯•è€… {num_subject}")
        continue

    args['target_subject'] = num_subject
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå—è¯•è€… S{num_subject:02d}")

    try:
        dataset = get_dataset(aargs.config_name, args)

        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        if len(dataset) == 0:
            print(f"âŒ å—è¯•è€… S{num_subject:02d} æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡")
            continue

        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")

    except Exception as e:
        print(f"âŒ å—è¯•è€… S{num_subject:02d} æ•°æ®åŠ è½½å‡ºé”™: {e}")
        print("â­ï¸  è·³è¿‡è¯¥å—è¯•è€…")
        continue

    kfold = KFold(n_splits=args.k_folds, shuffle=True, random_state=args.SEED)

    for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):
        # ä¿®æ”¹é€‰æ‹©é€»è¾‘ï¼šå¦‚æœfold_numæ˜¯é»˜è®¤å€¼0ï¼Œå°±è®­ç»ƒæ‰€æœ‰fold
        if aargs.fold_num != 0 and fold != aargs.fold_num:
            print(f"â­ï¸  è·³è¿‡Fold {fold}")
            continue

        print(f"ğŸ”„ Fold {fold + 1}/{args.k_folds}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_idx)}, éªŒè¯æ ·æœ¬: {len(val_idx)}")

        ### Set dataloader ###
        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)
        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)

        train_dataloader = DataLoader(dataset,
                                      batch_size=args.batch_size,
                                      pin_memory=False,
                                      num_workers=args.num_workers,
                                      sampler=train_subsampler)
        val_dataloader = DataLoader(dataset,
                                    batch_size=args.batch_size,
                                    pin_memory=False,
                                    num_workers=args.num_workers,
                                    sampler=val_subsampler)

        model = get_litmodel(args)
        logger = TensorBoardLogger(args.LOG_PATH,
                                   name=f'{args.LOG_NAME}/S{args.target_subject:02d}_fold{fold + 1}', version='')
        callbacks = get_callbacks(fold=fold, monitor='val_acc', args=args)


        # è‡ªå®šä¹‰è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºæ‰€æœ‰æŒ‡æ ‡ï¼ˆåŒ…å«Kappaï¼‰
        class DetailedProgressBar(TQDMProgressBar):
            def init_validation_tqdm(self):
                bar = super().init_validation_tqdm()
                bar.disable = True  # ç¦ç”¨éªŒè¯è¿›åº¦æ¡
                return bar

            def get_metrics(self, trainer, model):
                items = super().get_metrics(trainer, model)
                # ç§»é™¤v_numï¼Œæ·»åŠ è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
                items.pop("v_num", None)

                # æ·»åŠ è®­ç»ƒæŒ‡æ ‡
                if 'train_loss' in trainer.callback_metrics:
                    items['train_loss'] = f"{trainer.callback_metrics['train_loss']:.4f}"
                if 'train_acc' in trainer.callback_metrics:
                    items['train_acc'] = f"{trainer.callback_metrics['train_acc']:.4f}"
                if 'train_kappa' in trainer.callback_metrics:
                    items['train_kappa'] = f"{trainer.callback_metrics['train_kappa']:.4f}"

                # æ·»åŠ éªŒè¯æŒ‡æ ‡
                if 'val_loss' in trainer.callback_metrics:
                    items['val_loss'] = f"{trainer.callback_metrics['val_loss']:.4f}"
                if 'val_acc' in trainer.callback_metrics:
                    items['val_acc'] = f"{trainer.callback_metrics['val_acc']:.4f}"
                if 'val_kappa' in trainer.callback_metrics:
                    items['val_kappa'] = f"{trainer.callback_metrics['val_kappa']:.4f}"

                return items


        callbacks.append(DetailedProgressBar())

        # æ ¹æ®æ˜¯å¦ä½¿ç”¨GPUé…ç½®Trainer
        if use_gpu:
            trainer = Trainer(
                enable_progress_bar=True,
                max_epochs=args.EPOCHS,
                accelerator="gpu",
                devices=[int(aargs.gpu_num)],
                callbacks=callbacks,
                default_root_dir=args.CKPT_PATH,
                logger=logger,
                log_every_n_steps=20,
            )
        else:
            trainer = Trainer(
                enable_progress_bar=True,
                max_epochs=args.EPOCHS,
                accelerator="cpu",
                devices=1,  # CPUæ¨¡å¼ä¸‹è®¾ç½®ä¸º1
                callbacks=callbacks,
                default_root_dir=args.CKPT_PATH,
                logger=logger,
                log_every_n_steps=20,
            )

        # è®­ç»ƒå¹¶è·å–æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        trainer.fit(model,
                    train_dataloaders=train_dataloader,
                    val_dataloaders=val_dataloader)

        # è·å–æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_val_acc = trainer.checkpoint_callback.best_model_score.item()

        # è¿›è¡Œè¯¦ç»†è¯„ä¼°ï¼ˆè®¡ç®—Kappaç­‰æŒ‡æ ‡ï¼‰
        print("ğŸ“Š è¿›è¡Œè¯¦ç»†è¯„ä¼°...")
        detailed_metrics = calculate_detailed_metrics(model, val_dataloader, args.device)

        print(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœ:")
        print(f"   å‡†ç¡®ç‡ (Acc): {detailed_metrics['accuracy']:.4f}")
        print(f"   Kappaç³»æ•°: {detailed_metrics['kappa']:.4f}")
        print(f"   Kappaè§£è¯»: {interpret_kappa(detailed_metrics['kappa'])}")
        print(f"   æ··æ·†çŸ©é˜µ:\n{detailed_metrics['confusion_matrix']}")

        # ç”Ÿæˆè®­ç»ƒæ›²çº¿
        try:
            # ä¿®æ­£TensorBoardæ—¥å¿—è·¯å¾„æŸ¥æ‰¾
            log_dir = logger.log_dir
            print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")

            # æŸ¥æ‰¾metrics.csvæ–‡ä»¶
            csv_path = None
            for root, dirs, files in os.walk(log_dir):
                if 'metrics.csv' in files:
                    csv_path = os.path.join(root, 'metrics.csv')
                    break

            plot_paths = []
            if csv_path and os.path.exists(csv_path):
                print(f"âœ… æ‰¾åˆ°CSVæ–‡ä»¶: {csv_path}")
                plot_paths = plot_training_curves_from_csv(csv_path, num_subject, fold + 1, plot_dir)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°CSVæ—¥å¿—æ–‡ä»¶ï¼Œæœç´¢ç›®å½•: {log_dir}")
                # å°è¯•ç›´æ¥ä»æ¨¡å‹å†å²ç”Ÿæˆå›¾è¡¨
                plot_paths = generate_plot_from_model_history(model, num_subject, fold + 1, plot_dir)

        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆè®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
            plot_paths = []

        all_results.append({
            'subject': num_subject,
            'fold': fold,
            'val_acc': best_val_acc,  # æ¥è‡ªcheckpointçš„æœ€ä½³å‡†ç¡®ç‡
            'val_acc_final': detailed_metrics['accuracy'],  # æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡
            'val_kappa': detailed_metrics['kappa'],  # Kappaç³»æ•°
            'confusion_matrix': detailed_metrics['confusion_matrix'],  # æ··æ·†çŸ©é˜µ
            'plot_path': plot_paths
        })

        print(f"âœ… å—è¯•è€… S{num_subject:02d} Fold {fold + 1} å®Œæˆ")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {detailed_metrics['accuracy']:.4f}")
        print(f"   Kappaç³»æ•°: {detailed_metrics['kappa']:.4f}")

        torch.cuda.empty_cache()

# æ˜¾ç¤ºæ±‡æ€»ç»“æœ
print("\n" + "=" * 80)
print("ğŸ¯ è®­ç»ƒç»“æœæ±‡æ€» (åŒ…å«Kappaç³»æ•°)")
print("=" * 80)

if all_results:
    for result in all_results:
        print(f"å—è¯•è€… S{result['subject']:02d} Fold {result['fold'] + 1}:")
        print(f"  âœ… æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['val_acc']:.4f}")
        print(f"  âœ… æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {result['val_acc_final']:.4f}")
        print(f"  ğŸ“Š Kappaç³»æ•°: {result['val_kappa']:.4f}")
        print(f"  ğŸ“ Kappaè§£è¯»: {interpret_kappa(result['val_kappa'])}")
        if result['plot_path']:
            print(f"  ğŸ“ˆ å›¾è¡¨è·¯å¾„: {result['plot_path']}")
        print()

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_acc = sum(r['val_acc'] for r in all_results) / len(all_results)
    avg_acc_final = sum(r['val_acc_final'] for r in all_results) / len(all_results)
    avg_kappa = sum(r['val_kappa'] for r in all_results) / len(all_results)

    print(f"\nğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
    print(f"  âœ… å¹³å‡æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {avg_acc:.4f}")
    print(f"  âœ… å¹³å‡æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {avg_acc_final:.4f}")
    print(f"  ğŸ“Š å¹³å‡Kappaç³»æ•°: {avg_kappa:.4f}")
    print(f"  ğŸ“ å¹³å‡Kappaè§£è¯»: {interpret_kappa(avg_kappa)}")

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶
    results_csv_path = os.path.join(plot_dir, 'detailed_results.csv')
    results_df = pd.DataFrame([
        {
            'subject': r['subject'],
            'fold': r['fold'],
            'best_val_acc': r['val_acc'],
            'final_val_acc': r['val_acc_final'],
            'kappa': r['val_kappa'],
            'kappa_interpretation': interpret_kappa(r['val_kappa'])
        }
        for r in all_results
    ])
    results_df.to_csv(results_csv_path, index=False)
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_csv_path}")

else:
    print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœ")

print("=" * 80)